{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMf6DiSVK9oe9ocIZPcJkOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/drift/blob/main/MMD_0727.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yxnIq6yrdJM",
        "outputId": "f96f614b-ad2f-4d37-b28f-a4c687a22db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import random\n",
        "np.random.seed(1337)"
      ],
      "metadata": {
        "id": "fcgs9kqBrohz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int = 46):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "Fe6PfKOHrqfi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_accepted = pd.read_csv(\"/content/drive/MyDrive/Drift/accepted_2007_to_2018Q4.csv\")\n",
        "\n",
        "print(df_accepted.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3jy8Uqtrtbt",
        "outputId": "6d5bdb6c-939d-43d1-e165-14c4a8856e91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2260701, 151)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_accepted = df_accepted[df_accepted.loan_status.isin(['Fully Paid', 'Charged Off'])]\n",
        "cols_to_drop = df_accepted.isnull().sum().sort_values(ascending=False).head(50).index\n",
        "df_accepted = df_accepted.drop(columns=cols_to_drop)\n",
        "\n",
        "cols_to_drop = df_accepted.isnull().sum().sort_values(ascending=False).head(45).index\n",
        "df_accepted = df_accepted.drop(columns=cols_to_drop)\n",
        "\n",
        "df_accepted = df_accepted.dropna()\n",
        "\n",
        "cols_to_drop = ['out_prncp', 'out_prncp_inv', 'policy_code']\n",
        "df_accepted = df_accepted.drop(columns=cols_to_drop)\n",
        "\n",
        "cols_to_drop = ['fico_range_low', 'funded_amnt_inv', 'funded_amnt', 'total_pymnt_inv', 'total_pymnt', 'installment', 'collection_recovery_fee', 'total_rec_prncp', 'last_fico_range_low']\n",
        "df_accepted = df_accepted.drop(columns=cols_to_drop)\n",
        "\n",
        "values = df_accepted.loan_status.unique()\n",
        "encode = [1,0]\n",
        "d = dict(zip(values, encode))\n",
        "\n",
        "df_accepted['loan_paid'] = df_accepted['loan_status'].map(d)\n",
        "\n",
        "df_accepted = df_accepted.drop(columns=['loan_status'])\n",
        "\n",
        "cols_to_drop = ['id', 'pymnt_plan', 'hardship_flag']\n",
        "df_accepted = df_accepted.drop(columns=cols_to_drop)\n",
        "\n",
        "cat_cols = df_accepted.select_dtypes(include=['object']).columns\n",
        "print(cat_cols)\n",
        "\n",
        "print(df_accepted.term.value_counts())\n",
        "\n",
        "# convert term into either a 36 or 60 integer numeric data type\n",
        "\n",
        "l1 = df_accepted.term.unique()\n",
        "l2 = [36, 60]\n",
        "d = dict(zip(l1, l2))\n",
        "\n",
        "df_accepted['term'] = df_accepted['term'].map(d)\n",
        "\n",
        "# as grade is part of sub_grade, so let's just drop the grade feature.\n",
        "\n",
        "df_accepted = df_accepted.drop(columns='grade')\n",
        "\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['sub_grade'], prefix='', prefix_sep='', drop_first=True)\n",
        "\n",
        "cat_cols = df_accepted.select_dtypes(include=['object']).columns\n",
        "print(cat_cols)\n",
        "\n",
        "# for these columns, let's just create dummy variables, concatenate them with the original dataframe and drop original columns.\n",
        "\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['verification_status'], prefix='', prefix_sep='', drop_first=True)\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['application_type'], prefix='', prefix_sep='', drop_first=True)\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['initial_list_status'], prefix='', prefix_sep='', drop_first=True)\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['purpose'], prefix='', prefix_sep='', drop_first=True)\n",
        "\n",
        "df_accepted['home_ownership'] = df_accepted.home_ownership.replace('NONE', 'ANY')\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['home_ownership'], prefix='', prefix_sep='', drop_first=True)\n",
        "df_accepted = df_accepted.drop(columns=[ 'url', 'last_pymnt_d', 'last_credit_pull_d'])\n",
        "df_accepted = df_accepted.drop(columns=['zip_code', 'addr_state'])\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['disbursement_method'], prefix='', prefix_sep='', drop_first=True)\n",
        "df_accepted = pd.get_dummies(df_accepted, columns = ['debt_settlement_flag'], prefix='', prefix_sep='', drop_first=True)\n",
        "df_accepted = df_accepted.drop(columns=['earliest_cr_line'])\n",
        "\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Jan','01')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Feb','02')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Mar','03')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Apr','04')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('May','05')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Jun','06')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Jul','07')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Aug','08')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Sep','09')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Oct','10')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Dec','11')\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].str.replace('Nov','12')\n",
        "\n",
        "def reorder_issue_d(dt):\n",
        "    a = dt.split('-')\n",
        "    return a[1]+a[0]\n",
        "\n",
        "reorder_issue_d('01-2009')\n",
        "\n",
        "df_accepted['issue_d'] = df_accepted['issue_d'].apply(reorder_issue_d)\n",
        "df_accepted = df_accepted.sort_values(by='issue_d')\n",
        "df_accepted.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X = {}\n",
        "Y = {}\n",
        "\n",
        "df = df_accepted[df_accepted['issue_d'].str.startswith('2007') + df_accepted['issue_d'].str.startswith('2008') + df_accepted['issue_d'].str.startswith('2009')].copy()\n",
        "df = df.drop(columns=['issue_d'])\n",
        "\n",
        "X[2009] = df.loc[:, df.columns != 'loan_paid'].values\n",
        "Y[2009] = df.loan_paid.values\n",
        "\n",
        "for year in range(2010,2019):\n",
        "    print(year)\n",
        "    df = df_accepted[df_accepted['issue_d'].str.startswith(str(year))].copy()\n",
        "    df = df.drop(columns=['issue_d'])\n",
        "    X[year] = df.loc[:, df.columns != 'loan_paid'].values\n",
        "    Y[year] = df.loan_paid.values\n",
        "\n",
        "COLUMNS = df.loc[:, df.columns != 'loan_paid'].columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC-xZrifrt7l",
        "outputId": "0e1e0292-4dd3-4e92-efe7-c5d0b4a2046a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',\n",
            "       'issue_d', 'url', 'purpose', 'zip_code', 'addr_state',\n",
            "       'earliest_cr_line', 'initial_list_status', 'last_pymnt_d',\n",
            "       'last_credit_pull_d', 'application_type', 'disbursement_method',\n",
            "       'debt_settlement_flag'],\n",
            "      dtype='object')\n",
            " 36 months    1017398\n",
            " 60 months     323628\n",
            "Name: term, dtype: int64\n",
            "Index(['home_ownership', 'verification_status', 'issue_d', 'url', 'purpose',\n",
            "       'zip_code', 'addr_state', 'earliest_cr_line', 'initial_list_status',\n",
            "       'last_pymnt_d', 'last_credit_pull_d', 'application_type',\n",
            "       'disbursement_method', 'debt_settlement_flag'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/computation/expressions.py:215: UserWarning: evaluating in Python space because the '+' operator is not supported by numexpr for the bool dtype, use '|' instead\n",
            "  f\"evaluating in Python space because the {repr(op_str)} \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ML = {}\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "MODEL_TYPE = 'DL'  ## RF / DL\n",
        "\n",
        "def get_one_hot(targets, nb_classes):\n",
        "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
        "    return res.reshape(list(targets.shape)+[nb_classes])\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def build_model_rf(year,x,y,epochs=5):\n",
        "    X_train= scaler.fit_transform(x)\n",
        "    y_train = y #get_one_hot(y,2)\n",
        "    model = RandomForestClassifier(n_estimators=80,max_depth=5)\n",
        "    model.fit(X_train,y_train)\n",
        "    return model\n",
        "\n",
        "import os.path \n",
        "\n",
        "def build_model_dl(year,x,y,epochs=100):\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X[year], Y[year], test_size=0.2, random_state=42)\n",
        "    X_train= scaler.fit_transform(x)\n",
        "    #X_test = scaler.transform(X_test)\n",
        "    y_train = get_one_hot(y,2)\n",
        "\n",
        "    # building the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=78,activation='relu'))\n",
        "    model.add(Dense(units=39,activation='relu'))\n",
        "    model.add(Dense(units=19,activation='relu'))\n",
        "    model.add(Dense(units=8,activation='relu'))\n",
        "    model.add(Dense(units=4,activation='relu'))\n",
        "    model.add(Dense(units=2,activation='softmax'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    weight_path = '/content/drive/MyDrive/Drift/model/ml_'+str(year)\n",
        "    if os.path.exists(weight_path+'.index'):\n",
        "        model.load_weights(weight_path)\n",
        "        print('load weights from ',weight_path)\n",
        "    else:\n",
        "        model.fit(x=X_train, \n",
        "                y=y_train, \n",
        "                epochs=epochs,\n",
        "                batch_size=512,\n",
        "                verbose=1)\n",
        "            #validation_data=(X_test, y_test), verbose=1)\n",
        "        if year > 2000:\n",
        "            model.save_weights(weight_path) \n",
        "            print('save weights to ',weight_path)\n",
        "    return model"
      ],
      "metadata": {
        "id": "0KtpFOxIsmOM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything()\n",
        "\n",
        "offset = int(Y[2010].shape[0] * 0.7)\n",
        "y_ref = Y[2010][0:offset]\n",
        "x_ref = X[2010][0:offset]\n",
        "y_inf = Y[2010][offset:]\n",
        "x_inf = X[2010][offset:]\n",
        "\n",
        "ML[2010] = build_model_dl(year,x_ref,y_ref,epochs=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nde0XiG5ssiq",
        "outputId": "c23181ce-231c-4bd5-9d05-0b7a1b435d9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load weights from  /content/drive/MyDrive/Drift/model/ml_2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def predict(x):\n",
        "    return np.asarray([0 if r[0] > 0.5 else 1 for r in ML[2010].predict(scaler.transform(x))])\n",
        "\n",
        "base_score = precision_recall_fscore_support(y_inf, predict(x_inf) ,average='binary')\n",
        "base_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3nL0F2TzJBN",
        "outputId": "9929b6c1-817d-40ac-e4f0-9cf102783295"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.942577626541897, 0.7345044746436857, 0.8256333830104322, None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2010년의 data를 기준으로 그 이후의 data 들...\n",
        "X2 = np.concatenate((X[2011],X[2012],X[2013],X[2014],X[2015],X[2016],X[2017]))\n",
        "Y2 = np.concatenate((Y[2011],Y[2012],Y[2013],Y[2014],Y[2015],Y[2016],Y[2017]))\n",
        "\n",
        "print(\"trainset:\",X[2010].shape)\n",
        "print(\"online dataset:\",X2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdTrPN59s2FV",
        "outputId": "dad2abc3-db88-4709-93fe-ec22dcdc71e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainset: (11497, 81)\n",
            "online dataset: (1268183, 81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import sys\n",
        "import math\n",
        "if sys.version_info >= (3, 5):\n",
        "    from math import gcd\n",
        "else:\n",
        "    from fractions import gcd\n",
        "\n",
        "from scipy.stats import ks_2samp, kstest\n",
        "import statsmodels.api as sm # import statsmodels \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import random\n",
        "from statistics import mean\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCYb0EWsty4p",
        "outputId": "4af38523-97a6-4293-aade-ad6c3bce4425"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alibi-detect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBN6bSUKupSY",
        "outputId": "80e9e305-7d0f-463e-f794-7adc3402f2c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting alibi-detect\n",
            "  Downloading alibi_detect-0.10.0-py3-none-any.whl (390 kB)\n",
            "\u001b[K     |████████████████████████████████| 390 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image!=0.17.1,<0.20,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.18.3)\n",
            "Collecting toml<1.0.0,>=0.10.1\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.3.5)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (4.64.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.7.3)\n",
            "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.3.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (4.1.1)\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (4.6.0.66)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (2.23.0)\n",
            "Requirement already satisfied: catalogue<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (2.0.7)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.9.1)\n",
            "Requirement already satisfied: Pillow<10.0.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (7.1.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.21.6)\n",
            "Requirement already satisfied: numba!=0.54.0,<0.56.0,>=0.50.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.51.2)\n",
            "Collecting transformers<5.0.0,>=4.0.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 98.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<3.0.0,>=2.0.0->alibi-detect) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.4.4)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba!=0.54.0,<0.56.0,>=0.50.0->alibi-detect) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba!=0.54.0,<0.56.0,>=0.50.0->alibi-detect) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi-detect) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (3.0.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi-detect) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi-detect) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi-detect) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi-detect) (2.6.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0.0,>=0.20.2->alibi-detect) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0.0,>=0.20.2->alibi-detect) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (4.12.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 79.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 29.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyyaml, tokenizers, huggingface-hub, transformers, toml, alibi-detect\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed alibi-detect-0.10.0 huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 toml-0.10.2 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from alibi_detect.cd import MMDDrift, ContextMMDDrift\n",
        "from scipy.special import softmax\n",
        "\n",
        "# mmd drift detector\n",
        "dd_mmd = MMDDrift(x_ref, p_val=.05, n_permutations=100, backend='pytorch',sigma=0.9, configure_kernel_from_x_ref = False)\n",
        "\"\"\"\n",
        "def context(x: np.ndarray) -> np.ndarray:\n",
        "    logits = ML[2010].predict(scaler.transform(x))\n",
        "    return logits\n",
        "\n",
        "c_ref = context(x_ref) \n",
        "\n",
        "dd_cad = ContextMMDDrift(x_ref[:,0:2], c_ref, p_val=.05, batch_size=32, n_permutations=100, backend='pytorch')\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "nLuUu_rSvJ1Q",
        "outputId": "b7917af1-303b-4482-fa63-21bdde0dc909"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef context(x: np.ndarray) -> np.ndarray:\\n    logits = ML[2010].predict(scaler.transform(x))\\n    return logits\\n\\nc_ref = context(x_ref) \\n\\ndd_cad = ContextMMDDrift(x_ref[:,0:2], c_ref, p_val=.05, batch_size=32, n_permutations=100, backend='pytorch')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_ref.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUAGcXCCnL6K",
        "outputId": "3409cdbe-f31e-4bfd-e315-eb2d03266e8b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8047, 81)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(np.std(x_ref,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z62mJ-alnO-b",
        "outputId": "5166ac83-bf53-4ffb-fee0-80306faadda9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95751.30956530587"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in range(0,100):\n",
        "    # mmd drift detector\n",
        "    sigma=np.array(np.max(np.std(x_ref,0))) * 0.01 * p\n",
        "    dd_mmd = MMDDrift(x_ref, p_val=.05, n_permutations=100, backend='pytorch',sigma=sigma, configure_kernel_from_x_ref = False)\n",
        "    preds_mmd = dd_mmd.predict(x_inf)\n",
        "    print(sigma,preds_mmd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "bAC68HHDweWf",
        "outputId": "24f44219-6be5-4fb8-afde-a35f82ea207f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-0f9f99547e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdd_mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMMDDrift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_permutations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigure_kernel_from_x_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpreds_mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd_mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds_mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/mmd.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, return_p_val, return_distance)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;34m'data'\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdrift\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moptionally\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mMMD\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_p_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, return_p_val, return_distance)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \"\"\"\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# compute drift scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0mdrift_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_val\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/pytorch/mmd.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# TODO: (See https://github.com/SeldonIO/alibi-detect/issues/540)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mkernel_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mkernel_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_mat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zero diagonal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mmmd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmd2_from_kernel_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_diag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/pytorch/mmd.py\u001b[0m in \u001b[0;36mkernel_matrix\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mk_xx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_xx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_xx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_x_ref\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mk_yy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mkernel_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_xx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_xy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_xy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_yy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkernel_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1010.00 MiB (GPU 0; 15.90 GiB total capacity; 12.88 GiB already allocated; 743.75 MiB free; 14.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context(x_inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS-b3zfExN_s",
        "outputId": "16c8024f-2cfb-47c3-b55e-308c9587d6e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9765128e-01, 2.3487839e-03],\n",
              "       [2.7078291e-11, 1.0000000e+00],\n",
              "       [6.6567125e-08, 9.9999988e-01],\n",
              "       ...,\n",
              "       [2.1562046e-01, 7.8437954e-01],\n",
              "       [1.3817095e-09, 1.0000000e+00],\n",
              "       [9.6438534e-08, 9.9999988e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_cad = dd_cad.predict(x_inf[:,0:2], context(x_inf))\n",
        "preds_cad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AsXbLBXlwrMP",
        "outputId": "0a836bae-a1a9-4f11-9579-a8ef4816dc3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dc904122ea37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_cad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd_cad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_inf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreds_cad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/context_aware.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, c, return_p_val, return_distance, return_coupling)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mcoupling\u001b[0m \u001b[0mmatrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_p_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_coupling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, c, return_p_val, return_distance, return_coupling)\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \"\"\"\n\u001b[1;32m   1190\u001b[0m         \u001b[0;31m# compute drift scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoupling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m         \u001b[0mdrift_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_val\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/tensorflow/context_aware.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# Compute test stat on original and reassigned data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoupling_xx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoupling_yy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoupling_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_held\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_held\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mpermuted_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm_bools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_held\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_held\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mperm_bools\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/tensorflow/context_aware.py\u001b[0m in \u001b[0;36m_cmmd\u001b[0;34m(self, K, L, bools, L_held)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mpossible_lams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mlam_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_lam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_lams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mlam_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_lam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_lams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alibi_detect/cd/tensorflow/context_aware.py\u001b[0m in \u001b[0;36m_pick_lam\u001b[0;34m(self, lams, K, L, n_folds)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mn_if\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_if\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             L_inv_lams = tf.stack(\n\u001b[0;32m--> 241\u001b[0;31m                 [tf.linalg.inv(L_if + n_if*lam*tf.eye(n_if, dtype=tf.float64)) for lam in lams])  # n_lam x n_if x n_if\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mKW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ij,ljk->lik'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_if\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_inv_lams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mlW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ij,ljk->lik'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds_oof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds_if\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_inv_lams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,6438,6438] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Pack] name: stack"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df = {}\n",
        "\n",
        "drift_d = []\n",
        "df['precision'] = []\n",
        "df['dd_mmd_statistic'] = []\n",
        "df['dd_mmd_p_value'] = []\n",
        "#df['dd_cad_statistic'] = []\n",
        "#df['dd_cad_p_value'] = []\n",
        "\n",
        "beta = 0\n",
        "window_size = 10000\n",
        "step = 2000\n",
        "batch = math.ceil((X2.shape[0]-window_size)/step)\n",
        "for i in range(batch):\n",
        "    ux = X2[i*step:i*step+window_size]\n",
        "    uy = Y2[i*step:i*step+window_size]\n",
        "    f1 = precision_recall_fscore_support(uy, predict(ux),average='binary')\n",
        "\n",
        "    df['precision'].append(f1[0])\n",
        "\n",
        "    preds_mmd = dd_mmd.predict(ux)\n",
        "    df['dd_mmd_p_value'].append(preds_mmd['data']['p_val'])    \n",
        "    df['dd_mmd_statistic'].append(preds_mmd['data']['distance'])    \n",
        "\n",
        "df2 = pd.DataFrame(df)\n",
        "\n",
        "df2['dd_mmd_statistic'].plot(legend=True,figsize=(20,5))\n",
        "plt.show()\n",
        "df2['dd_mmd_p_value'].plot(legend=True,figsize=(20,5))\n",
        "plt.show()\n",
        "df2['precision'].plot(legend=True,figsize=(20,5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g_19-0nOtVjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['dd_cad_statistic']\n",
        "del df['dd_cad_p_value']\n",
        "df2 = pd.DataFrame(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "aikoBoMfIb3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['dd_mmd_statistic'].plot(legend=True,figsize=(20,5))\n",
        "plt.show()\n",
        "df2['dd_mmd_p_value'].plot(legend=True,figsize=(20,5))\n",
        "plt.show()\n",
        "df2['precision'].plot(legend=True,figsize=(20,5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fs4EIwviIxU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}