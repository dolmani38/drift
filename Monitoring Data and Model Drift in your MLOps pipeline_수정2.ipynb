{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNO4o82cf5Ez/0lGQFqJRKn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/drift/blob/main/Monitoring%20Data%20and%20Model%20Drift%20in%20your%20MLOps%20pipeline_%EC%88%98%EC%A0%952.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kydh5rq0gtRv"
      },
      "source": [
        "# Monitoring Data and Model Drift in your MLOps pipeline\n",
        "\n",
        "https://www.persistent.com/blogs/monitoring-data-and-model-drift-in-your-mlops-pipeline/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKLfh2HTgyOF",
        "outputId": "70905f34-eff9-4084-f1e0-bb3e0fa020bb"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mriy3v-hbEP",
        "outputId": "e0dd6174-dbac-4361-ab77-c55d7789cb36"
      },
      "source": [
        "!git clone https://github.com/sj2503/drift-detection-project"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'drift-detection-project'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 31 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc5zdKfwh5tF"
      },
      "source": [
        "# Installing and Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCm-9QIChz3l",
        "outputId": "14423bb6-4b05-4130-fc78-84b4f2e2f92b"
      },
      "source": [
        "!pip install mlflow\n",
        "!pip install shap\n",
        "!pip install evidently\n",
        "!pip install alibi-detect"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-1.20.2-py3-none-any.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 72.0 MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (4.6.4)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.2-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.0)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2021.5.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (2.4.7)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
            "Building wheels for collected packages: alembic, databricks-cli, prometheus-flask-exporter\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=fff682aaa7ea56fabb95a80fc7f85e530b652cf82fb6b0cb65a85f883696c8cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105260 sha256=1af22c3bc8b61291eaa269c5e255e56562d0bd698a70e9c72b55491f206a14a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17415 sha256=d335fd3fc5cb5e306542fb094e644dccec8bd1cc919212b5bb9a076e1fce8a55\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n",
            "Successfully built alembic databricks-cli prometheus-flask-exporter\n",
            "Installing collected packages: smmap, websocket-client, python-editor, Mako, gitdb, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, alembic, mlflow\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.1.5 alembic-1.4.1 databricks-cli-0.15.0 docker-5.0.2 gitdb-4.0.7 gitpython-3.1.18 gunicorn-20.1.0 mlflow-1.20.2 prometheus-flask-exporter-0.18.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 smmap-4.0.0 websocket-client-1.2.1\n",
            "Collecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.62.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491649 sha256=8d4519e5b75192fc0e78c55076dd33283c5fe5602609d9ec0eec7b7f9685af5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n",
            "Collecting evidently\n",
            "  Downloading evidently-0.1.24.dev0-py3-none-any.whl (15.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.2 MB 120 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from evidently) (1.1.5)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from evidently) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from evidently) (5.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from evidently) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy<=1.6.3 in /usr/local/lib/python3.7/dist-packages (from evidently) (1.4.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from evidently) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from evidently) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from evidently) (4.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->evidently) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evidently) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->evidently) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->evidently) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->evidently) (1.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->evidently) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->evidently) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->evidently) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->evidently) (3.0.4)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->evidently) (0.5.1)\n",
            "Installing collected packages: dataclasses, evidently\n",
            "Successfully installed dataclasses-0.6 evidently-0.1.24.dev0\n",
            "Collecting alibi-detect\n",
            "  Downloading alibi_detect-0.7.2-py3-none-any.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.19.5)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.4.1)\n",
            "Requirement already satisfied: Pillow<9.0.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (7.1.2)\n",
            "Collecting tensorflow-probability<0.13.0,>=0.8.0\n",
            "  Downloading tensorflow_probability-0.12.2-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8 MB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python<5.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.22.2.post1)\n",
            "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.3.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (4.62.0)\n",
            "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (1.1.5)\n",
            "Requirement already satisfied: scikit-image!=0.17.1,<0.19,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (0.16.2)\n",
            "Collecting tensorflow<2.6.0,>=2.0.0\n",
            "  Downloading tensorflow-2.5.1-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 8.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi-detect) (3.2.2)\n",
            "Collecting transformers<5.0.0,>=4.0.0\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi-detect) (2018.9)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi-detect) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi-detect) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi-detect) (2.6.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.20.2->alibi-detect) (1.0.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.1.2)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.12)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (2.6.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.12.1)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 29.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.12.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.4.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.34.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi-detect) (1.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi-detect) (0.1.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability<0.13.0,>=0.8.0->alibi-detect) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.0.0->alibi-detect) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.0.0->alibi-detect) (7.1.2)\n",
            "Installing collected packages: grpcio, tokenizers, tensorflow-estimator, sacremoses, keras-nightly, huggingface-hub, transformers, tensorflow-probability, tensorflow, alibi-detect\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.39.0\n",
            "    Uninstalling grpcio-1.39.0:\n",
            "      Successfully uninstalled grpcio-1.39.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.13.0\n",
            "    Uninstalling tensorflow-probability-0.13.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "Successfully installed alibi-detect-0.7.2 grpcio-1.34.1 huggingface-hub-0.0.17 keras-nightly-2.5.0.dev2021032900 sacremoses-0.0.45 tensorflow-2.5.1 tensorflow-estimator-2.5.0 tensorflow-probability-0.12.2 tokenizers-0.10.3 transformers-4.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd19aSeOiFNO",
        "outputId": "b002321a-f517-4256-8322-c2fc38c7de2b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.io.feather_format import read_feather\n",
        "import seaborn as sns\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from evidently.dashboard import Dashboard\n",
        "from evidently.tabs import ClassificationPerformanceTab, DataDriftTab\n",
        "\n",
        "import mlflow\n",
        "import shap\n",
        "import sys\n",
        "from alibi_detect.cd import ChiSquareDrift, KSDrift"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edPyUcGOjTBJ"
      },
      "source": [
        "# Defining mlfow_run Function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXpBIPwZjVI1"
      },
      "source": [
        "from alibi_detect.cd import KSDrift\n",
        "\n",
        "def create_mlflow_run():\n",
        "        print()\n",
        "        print(\"MLFlow run started...\")\n",
        "        # get the dataset name\n",
        "        p_dataset = 'data.csv'\n",
        "        p_model = sys.argv[4] if len(sys.argv) > 4 else 'RandomForestClassifier'\n",
        "        print()\n",
        "        print(\"Loading dataset %s ...\"%p_dataset)\n",
        "\n",
        "        # ##### LOAD THE DATA #####\n",
        "        data = pd.read_csv(p_dataset)\n",
        "        print() \n",
        "        print(\"Pre-processing & Cleansing...\")\n",
        "        data = data.drop(columns=['Loan_ID']) ## Dropping Loan ID\n",
        "        #categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n",
        "        categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Loan_Amount_Term']\n",
        "        numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
        "        numerical_data = data[numerical_columns].copy()\n",
        "        categorical_data = data[categorical_columns].copy()\n",
        "        categorical_data[\"Gender\"] = categorical_data[\"Gender\"].map({\"Male\":1,\"Female\":0})\n",
        "        categorical_data[\"Married\"] = categorical_data[\"Married\"].map({\"Yes\":1,\"No\":0})\n",
        "        categorical_data[\"Dependents\"] = categorical_data[\"Dependents\"].map({\"0\":0,\"1\":1,\"2\":2,\"3+\":3})\n",
        "        categorical_data[\"Education\"] = categorical_data[\"Education\"].map({\"Graduate\":1, \"Not Graduate\":0})\n",
        "        categorical_data[\"Self_Employed\"] = categorical_data[\"Self_Employed\"].map({\"Yes\":1, \"No\":0})\n",
        "        categorical_data[\"Loan_Amount_Term\"] = categorical_data[\"Loan_Amount_Term\"].map({12:0,36:1,60:2,84:3,120:4,180:5,240:6,300:7,360:8,480:9})\n",
        "        #categorical_data[\"Property_Area\"] = categorical_data[\"Property_Area\"].map({\"Urban\":2,\"Semiurban\":1, \"Rural\":0})\n",
        "        categorical_data = categorical_data.to_numpy().astype(int)\n",
        "\n",
        "        '''\n",
        "        # ##### PLOT CATEGORICAL COLUMNS #####\n",
        "        sns.set(style=\"white\", context=\"talk\")\n",
        "        sns.color_palette(\"rocket\")\n",
        "        fig,axes = plt.subplots(4,2,figsize=(15,25))\n",
        "        for idx,cat_col in enumerate(categorical_columns):\n",
        "            row,col = idx//2,idx%2\n",
        "            sns.countplot(x=cat_col, data=data, hue='Loan_Status', ax=axes[row,col])\n",
        "        plt.subplots_adjust(hspace=0.5)\n",
        "        fig.savefig('CATEGORICAL_DATA.png', bbox_inches='tight')\n",
        "\n",
        "        # ##### PLOT NUMERIC COLUMNS #####\n",
        "        fig,axes = plt.subplots(1,3,figsize=(17,5))\n",
        "        for idx,cat_col in enumerate(numerical_columns):\n",
        "            sns.boxplot(y=cat_col, data=data, x='Loan_Status',ax=axes[idx])\n",
        "        plt.subplots_adjust(hspace=0.5)\n",
        "        fig.savefig('NUMERIC_DATA.png', bbox_inches='tight')\n",
        "        '''\n",
        "        ##### FEATURE ENGINEERING #####\n",
        "        print()\n",
        "        print(\"Feature Engineering...\")\n",
        "        train_df_encoded = pd.get_dummies(data,drop_first=True)\n",
        "        X = train_df_encoded.drop(columns=['Loan_Status_Y','Credit_History','Property_Area_Se'])\n",
        "        X_columns = X.columns\n",
        "        y = train_df_encoded['Loan_Status_Y']\n",
        "        X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "        imp = SimpleImputer(strategy='mean')\n",
        "        imp_train = imp.fit(X_train)\n",
        "        X_train = imp_train.transform(X_train)\n",
        "        X_test_tmp = X_test.copy()\n",
        "        X_test = imp_train.transform(X_test)\n",
        "        X_test_df = pd.DataFrame(X_test, columns=X_columns)\n",
        "        imp_numerical_data = imp.fit(numerical_data)\n",
        "        numerical_data = imp_numerical_data.transform(numerical_data)\n",
        "\n",
        "        # ##### BUILD ML MODEL #####\n",
        "        print()\n",
        "        print(\"Building the ML Model...\")\n",
        "        model = RandomForestClassifier(n_estimators=50,max_depth=5)\n",
        "        model.fit(X_train,y_train)\n",
        "        # print(list(zip(train_df_encoded.columns, model.feature_importances_)))       \n",
        "        # cross-validation\n",
        "        xval_scores = cross_val_score(model, X_train, y_train, cv=10)   \n",
        "\n",
        "        # ##### METRICS #####\n",
        "        print()\n",
        "        print(\"Model evaluation...\")\n",
        "        y_pred = model.predict(X_train)\n",
        "        train_f1 = f1_score(y_train,y_pred)\n",
        "        train_acc = accuracy_score(y_train,y_pred)\n",
        "        y_pred = model.predict(X_test)\n",
        "        test_f1 = f1_score(y_test,y_pred)\n",
        "        test_acc = accuracy_score(y_test,y_pred)\n",
        "\n",
        "        # ##### LOG THE RUN #####\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_param('DATA CATALOG ENTRY', p_dataset)\n",
        "            mlflow.log_param('MODEL CATALOG ENTRY', p_model)\n",
        "            #mlflow.log_artifact('CATEGORICAL_DATA.png')\n",
        "            #mlflow.log_artifact('NUMERIC_DATA.png')\n",
        "            mlflow.log_artifact('CONFUSION_MATRIX.png')\n",
        "            mlflow.log_metric('TRAINING F1 SCORE', train_f1)\n",
        "            mlflow.log_metric('TESTING F1 SCORE', test_f1)\n",
        "            mlflow.log_metric('TRAINING ACCURACY', train_acc)\n",
        "            mlflow.log_metric('TESTING ACCURACY', test_acc)\n",
        "\n",
        "            print('--'*10)\n",
        "            print('TRAINING ACCURACY', train_acc)\n",
        "            print('TESTING ACCURACY', test_acc)\n",
        "            print('--'*10)\n",
        "            for score in xval_scores:\n",
        "                mlflow.log_metric('CROSS VALIDATION SCORE', score)\n",
        "            mlflow.log_artifact('SHAP_EXPLANATIONS.png')\n",
        "            print()\n",
        "            print(\"MLFlow run completed!\")\n",
        "\n",
        "        result_data = {}\n",
        "        result_data['model perf'] = []\n",
        "        result_data['ApplicantIncome df'] = []\n",
        "        result_data['CoapplicantIncome df'] = []\n",
        "        result_data['LoanAmount df'] = []\n",
        "\n",
        "        for step in range(1,200):\n",
        "\n",
        "            RG = 200000/step\n",
        "            # ##### DRIFT GENERATOR DATASET ######\n",
        "            print()\n",
        "            print(\"Creating dataset with data drift...\")\n",
        "            X_test_df_old = X_test_df.copy()\n",
        "            for i in range(0,X_test_df.shape[0]):\n",
        "                    ApplicantIncome_Change = np.random.randint(-60,-20)\n",
        "                    LoanAmount_Change = np.random.randint(30,71)\n",
        "                    CoapplicantIncome_Change = np.random.randint(-60,-20)\n",
        "                    X_test_df.loc[i,\"ApplicantIncome\"] = (1 + (ApplicantIncome_Change/RG)) * X_test_df.loc[i,\"ApplicantIncome\"]\n",
        "                    if(X_test_df.loc[i,\"CoapplicantIncome\"]!=0):\n",
        "                            X_test_df.loc[i,\"CoapplicantIncome\"] = (1 + (CoapplicantIncome_Change/RG)) * X_test_df.loc[i,\"CoapplicantIncome\"]\n",
        "                    X_test_df.loc[i,\"LoanAmount\"] = (1 + (LoanAmount_Change/RG)) * X_test_df.loc[i,\"LoanAmount\"]\n",
        "\n",
        "            # data drift detect...\n",
        "            cd = KSDrift(numerical_data, p_val=0.05)\n",
        "            preds = cd.predict(X_test_df[numerical_columns].to_numpy(), drift_type='batch', return_p_val=True, return_distance=True)\n",
        "            fpreds = cd.predict(X_test_df[numerical_columns].to_numpy(), drift_type='feature')\n",
        "            for f in range(cd.n_features):\n",
        "                stat = 'K-S'\n",
        "                fname = numerical_columns[f]\n",
        "                is_drift = fpreds['data']['is_drift'][f]\n",
        "                stat_val, p_val = preds['data']['distance'][f], preds['data']['p_val'][f]\n",
        "                print(f'{fname}-- Drift? {[is_drift]} -- {stat} {stat_val:.3f} -- p-value {p_val:.5f}') \n",
        "                result_data[fname + ' df'].append(stat_val)\n",
        "\n",
        "\n",
        "            y_pred_datadrift = model.predict(X_test_df)\n",
        "            datadrift_test_acc = accuracy_score(y_test,y_pred_datadrift)\n",
        "            print(\"Datadrift_Test_Accuracy: \",datadrift_test_acc)\n",
        "            result_data['model perf'].append(datadrift_test_acc)\n",
        "\n",
        "\n",
        "        result_df = pd.DataFrame(result_data)\n",
        "        result_df.plot()\n",
        "\n",
        "        X_test_df.to_csv(\"datadrift_inputdata_oldmodel.csv\")\n",
        "        print()\n",
        "        print(\"Generating data drift related reports...\")\n",
        "        loan_data_drift_report = Dashboard(tabs=[DataDriftTab])\n",
        "        loan_data_drift_report.calculate(X_test_df_old, X_test_df, column_mapping=None)\n",
        "        loan_data_drift_report.save(\"loan_datadrift_oldone.html\")\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu4PIbFijoxG"
      },
      "source": [
        "# Defining Function to Assign Ground Truth Values to Concept Drift Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn9Yf7bLjrX8"
      },
      "source": [
        "def GroundTruthValue(ApplicantIncome, LoanAmount, ApplicantIncome_Change, LoanAmount_Change):\n",
        "        if(ApplicantIncome>10000 and LoanAmount <=1000):\n",
        "                return 1\n",
        "        elif(ApplicantIncome<2500 and LoanAmount >=250):\n",
        "                return 0\n",
        "        elif(ApplicantIncome>=7500 and ApplicantIncome_Change > -10 and LoanAmount_Change < 45 and LoanAmount<400):\n",
        "                return 1\n",
        "        elif(ApplicantIncome<5000 and ApplicantIncome_Change < -5 and LoanAmount_Change > 35 and LoanAmount > 250):\n",
        "                return 0\n",
        "        elif(ApplicantIncome_Change > 0 and LoanAmount_Change < 30 and ApplicantIncome > 5000):\n",
        "                return 1\n",
        "        elif(ApplicantIncome_Change < -5 and LoanAmount_Change > 25 and ApplicantIncome < 3500):\n",
        "                return 0\n",
        "        else:\n",
        "                return 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5rbcwQ7jt-F"
      },
      "source": [
        "# Defining Function for New Input Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PbWFNo8jwcZ"
      },
      "source": [
        "def newinputdata(filename, input_dataframe, numerical_data, categorical_data):\n",
        "    f =open(filename,\"r\")\n",
        "    data = json.loads(f.read())\n",
        "    newdataframe = pd.DataFrame.from_dict([data])\n",
        "    input_dataframe = pd.concat([input_dataframe,newdataframe], axis=0, ignore_index=True)\n",
        "    if(input_dataframe.shape[0]>=30):\n",
        "        datadriftdetection(input_dataframe, numerical_data, categorical_data)\n",
        "    return input_dataframe"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcy_7iEZj2BF"
      },
      "source": [
        "# Defining Data Drift Detection Function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKHImp6j3OX"
      },
      "source": [
        "def datadriftdetection(input_dataframe, X_ref, categorical_data):\n",
        "    print()\n",
        "    print(\"Checking for Data Drift...\")\n",
        "    print()\n",
        "    print(\"Numerical Data...\")\n",
        "    print()\n",
        "    numeric_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
        "    numerical_input_dataframe = input_dataframe[numeric_columns].copy()\n",
        "    numerical_input_dataframe_numpy = numerical_input_dataframe.to_numpy()\n",
        "    categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n",
        "    categorical_input_dataframe = input_dataframe[categorical_columns].copy()\n",
        "    categorical_input_dataframe[\"Gender\"] = categorical_input_dataframe[\"Gender\"].map({\"Male\":1,\"Female\":0})\n",
        "    categorical_input_dataframe[\"Married\"] = categorical_input_dataframe[\"Married\"].map({\"Yes\":1,\"No\":0})\n",
        "    categorical_input_dataframe[\"Dependents\"] = categorical_input_dataframe[\"Dependents\"].map({\"0\":0,\"1\":1,\"2\":2,\"3+\":3})\n",
        "    categorical_input_dataframe[\"Education\"] = categorical_input_dataframe[\"Education\"].map({\"Graduate\":1, \"Not Graduate\":0})\n",
        "    categorical_input_dataframe[\"Self_Employed\"] = categorical_input_dataframe[\"Self_Employed\"].map({\"Yes\":1, \"No\":0})\n",
        "    categorical_input_dataframe[\"Loan_Amount_Term\"] = categorical_input_dataframe[\"Loan_Amount_Term\"].map({12:0,36:1,60:2,84:3,120:4,180:5,240:6,300:7,360:8,480:9})\n",
        "    categorical_input_dataframe[\"Property_Area\"] = categorical_input_dataframe[\"Property_Area\"].map({\"Urban\":2,\"Semiurban\":1, \"Rural\":0})\n",
        "    categorical_input_dataframe_numpy = categorical_input_dataframe.to_numpy().astype(int)\n",
        "    cd = KSDrift(X_ref, p_val=.05)\n",
        "    preds = cd.predict(numerical_input_dataframe_numpy, drift_type='feature', return_p_val=True, return_distance=True)\n",
        "    fpreds = cd.predict(numerical_input_dataframe_numpy, drift_type='feature')\n",
        "    print(preds)\n",
        "    print()\n",
        "    for f in range(cd.n_features):\n",
        "        stat = 'K-S'\n",
        "        fname = numeric_columns[f]\n",
        "        is_drift = fpreds['data']['is_drift'][f]\n",
        "        stat_val, p_val = preds['data']['distance'][f], preds['data']['p_val'][f]\n",
        "        print(f'{fname}-- Drift? {[is_drift]} -- {stat} {stat_val:.3f} -- p-value {p_val:.5f}')   \n",
        "\n",
        "    print()\n",
        "    print(\"Categorical Data\")\n",
        "    print()\n",
        "    cd = ChiSquareDrift(categorical_data, p_val=.05)\n",
        "    preds = cd.predict(categorical_input_dataframe_numpy)\n",
        "    print(preds)\n",
        "    print()\n",
        "    print(f\"Threshold {preds['data']['threshold']}\")\n",
        "    stat = \"Chi2\"\n",
        "    print()\n",
        "    for f in range(cd.n_features):\n",
        "        fname = categorical_columns[f]\n",
        "        is_drift = (preds['data']['p_val'][f] < preds['data']['threshold']).astype(int)\n",
        "        stat_val, p_val = preds['data']['distance'][f], preds['data']['p_val'][f]\n",
        "        print(f'{fname} -- Drift? {[is_drift]} -- {stat} {stat_val:.3f} -- p-value {p_val:.3f}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI3wz2XIkBvE"
      },
      "source": [
        "# Starting the mlflow_run Function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "E_L5FTu6kCld",
        "outputId": "cda211d3-40e5-4cbc-9e7a-4cd1bee75577"
      },
      "source": [
        "%cd /content/drift-detection-project/Loan Prediction Model\n",
        "create_mlflow_run()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drift-detection-project/Loan Prediction Model\n",
            "\n",
            "MLFlow run started...\n",
            "\n",
            "Loading dataset data.csv ...\n",
            "\n",
            "Pre-processing & Cleansing...\n",
            "\n",
            "Feature Engineering...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-2ae0f73888a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd /content/drift-detection-project/Loan Prediction Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcreate_mlflow_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-014abe8738f6>\u001b[0m in \u001b[0;36mcreate_mlflow_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature Engineering...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtrain_df_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Loan_Status_Y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Credit_History'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Property_Area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mX_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Loan_Status_Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Property_Area'] not found in axis\""
          ]
        }
      ]
    }
  ]
}